{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import coloredlogs\n",
    "from FAdo.conversions import *\n",
    "\n",
    "from utils.data_loader import *\n",
    "from utils.heuristics import *\n",
    "\n",
    "from alpha_zero.Coach import Coach\n",
    "from alpha_zero.MCTS import MCTS\n",
    "from alpha_zero.utils import *\n",
    "from alpha_zero.state_elimination.StateEliminationGame import StateEliminationGame as Game\n",
    "from alpha_zero.state_elimination.pytorch.NNet import NNetWrapper as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "coloredlogs.install(level='INFO')\n",
    "args = dotdict({\n",
    "    'numIters': 1000,\n",
    "    # Number of complete self-play games to simulate during a new iteration.\n",
    "    'numEps': 100,\n",
    "    'tempThreshold': 0,        # temperature hyperparameters\n",
    "    # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "    'updateThreshold': 0.6,\n",
    "    # Number of game examples to train the neural networks.\n",
    "    'maxlenOfQueue': 200000,\n",
    "    'numMCTSSims': 25,          # Number of games moves for MCTS to simulate.\n",
    "    # Number of games to play during arena play to determine if new net will be accepted.\n",
    "    'arenaCompare': 40,\n",
    "    'cpuct': 1,\n",
    "    'checkpoint': './alpha_zero/models/',\n",
    "    'load_model': True,\n",
    "    'load_folder_file': ('./alpha_zero/models/', 'best.pth.tar'),\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "})\n",
    "min_n = 3\n",
    "max_n = 7\n",
    "n_range = max_n - min_n + 1\n",
    "alphabet = [2, 5]\n",
    "density = [0.1, 0.2]\n",
    "sample_size = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_heuristics():\n",
    "    if os.path.isfile('./result/heuristics_experiment_result.pkl'):\n",
    "        with open('./result/heuristics_experiment_result.pkl', 'rb') as fp:\n",
    "            exp = load(fp)\n",
    "            return exp\n",
    "    else:\n",
    "        data = load_data('nfa')\n",
    "        exp = [[[[[0, 0] for d in range(len(density))] for k in range(\n",
    "            len(alphabet))] for n in range(n_range)] for c in range(6)]\n",
    "        for n in range(n_range):\n",
    "            for k in range(len(alphabet)):\n",
    "                for d in range(len(density)):\n",
    "                    for i in range(sample_size):\n",
    "                        random.seed(i)\n",
    "                        print('n' + str(n + min_n) + 'k' + ('2' if not k else ('5' if k == 1 else '10')) + 'd' + str(density[d])+ '\\'s ' + str(i + 1) + ' sample')\n",
    "                        # eliminate_randomly\n",
    "                        gfa = data[n][k][d][i].dup()\n",
    "                        start_time = time.time()\n",
    "                        result = eliminate_randomly(gfa)\n",
    "                        end_time = time.time()\n",
    "                        result_time = end_time - start_time\n",
    "                        result_size = result.treeLength()\n",
    "                        exp[0][n][k][d][0] += result_time\n",
    "                        exp[0][n][k][d][1] += result_size\n",
    "\n",
    "                        # decompose with eliminate_randomly\n",
    "                        gfa = data[n][k][d][i].dup()\n",
    "                        start_time = time.time()\n",
    "                        result = decompose(gfa, False, False)\n",
    "                        end_time = time.time()\n",
    "                        result_time = end_time - start_time\n",
    "                        result_size = result.treeLength()\n",
    "                        exp[1][n][k][d][0] += result_time\n",
    "                        exp[1][n][k][d][1] += result_size\n",
    "\n",
    "                        # eliminate_by_state_weight_heuristic\n",
    "                        gfa = data[n][k][d][i].dup()\n",
    "                        start_time = time.time()\n",
    "                        result = eliminate_by_state_weight_heuristic(gfa)\n",
    "                        end_time = time.time()\n",
    "                        result_time = end_time - start_time\n",
    "                        result_size = result.treeLength()\n",
    "                        exp[2][n][k][d][0] += result_time\n",
    "                        exp[2][n][k][d][1] += result_size\n",
    "\n",
    "                        # decompose + eliminate_by_state_weight_heuristic\n",
    "                        gfa = data[n][k][d][i].dup()\n",
    "                        start_time = time.time()\n",
    "                        result = decompose(gfa, True, False)\n",
    "                        end_time = time.time()\n",
    "                        result_time = end_time - start_time\n",
    "                        result_size = result.treeLength()\n",
    "                        exp[3][n][k][d][0] += result_time\n",
    "                        exp[3][n][k][d][1] += result_size\n",
    "\n",
    "                        # eliminate_by_repeated_state_weight_heuristic\n",
    "                        gfa = data[n][k][d][i].dup()\n",
    "                        start_time = time.time()\n",
    "                        result = eliminate_by_repeated_state_weight_heuristic(\n",
    "                            gfa)\n",
    "                        end_time = time.time()\n",
    "                        result_time = end_time - start_time\n",
    "                        result_size = result.treeLength()\n",
    "                        exp[4][n][k][d][0] += result_time\n",
    "                        exp[4][n][k][d][1] += result_size\n",
    "\n",
    "                        # decompose + eliminate_by_repeated_state_weight_heuristic\n",
    "                        gfa = data[n][k][d][i].dup()\n",
    "                        start_time = time.time()\n",
    "                        result = decompose(gfa, True, True)\n",
    "                        end_time = time.time()\n",
    "                        result_time = end_time - start_time\n",
    "                        result_size = result.treeLength()\n",
    "                        exp[5][n][k][d][0] += result_time\n",
    "                        exp[5][n][k][d][1] += result_size\n",
    "        with open('./result/heuristics_experiment_result.pkl', 'wb') as fp:\n",
    "            dump(exp, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n3k2d0.1's 1 sample\n",
      "n3k2d0.1's 2 sample\n",
      "n3k2d0.1's 3 sample\n",
      "n3k2d0.1's 4 sample\n",
      "n3k2d0.1's 5 sample\n",
      "n3k2d0.1's 6 sample\n",
      "n3k2d0.1's 7 sample\n",
      "n3k2d0.1's 8 sample\n",
      "n3k2d0.1's 9 sample\n",
      "n3k2d0.1's 10 sample\n",
      "n3k2d0.1's 11 sample\n",
      "n3k2d0.1's 12 sample\n",
      "n3k2d0.1's 13 sample\n",
      "n3k2d0.1's 14 sample\n",
      "n3k2d0.1's 15 sample\n",
      "n3k2d0.1's 16 sample\n",
      "n3k2d0.1's 17 sample\n",
      "n3k2d0.1's 18 sample\n",
      "n3k2d0.1's 19 sample\n",
      "n3k2d0.1's 20 sample\n",
      "n3k2d0.1's 21 sample\n",
      "n3k2d0.1's 22 sample\n",
      "n3k2d0.1's 23 sample\n",
      "n3k2d0.1's 24 sample\n",
      "n3k2d0.1's 25 sample\n",
      "n3k2d0.1's 26 sample\n",
      "n3k2d0.1's 27 sample\n",
      "n3k2d0.1's 28 sample\n",
      "n3k2d0.1's 29 sample\n",
      "n3k2d0.1's 30 sample\n",
      "n3k2d0.2's 1 sample\n",
      "n3k2d0.2's 2 sample\n",
      "n3k2d0.2's 3 sample\n",
      "n3k2d0.2's 4 sample\n",
      "n3k2d0.2's 5 sample\n",
      "n3k2d0.2's 6 sample\n",
      "n3k2d0.2's 7 sample\n",
      "n3k2d0.2's 8 sample\n",
      "n3k2d0.2's 9 sample\n",
      "n3k2d0.2's 10 sample\n",
      "n3k2d0.2's 11 sample\n",
      "n3k2d0.2's 12 sample\n",
      "n3k2d0.2's 13 sample\n",
      "n3k2d0.2's 14 sample\n",
      "n3k2d0.2's 15 sample\n",
      "n3k2d0.2's 16 sample\n",
      "n3k2d0.2's 17 sample\n",
      "n3k2d0.2's 18 sample\n",
      "n3k2d0.2's 19 sample\n",
      "n3k2d0.2's 20 sample\n",
      "n3k2d0.2's 21 sample\n",
      "n3k2d0.2's 22 sample\n",
      "n3k2d0.2's 23 sample\n",
      "n3k2d0.2's 24 sample\n",
      "n3k2d0.2's 25 sample\n",
      "n3k2d0.2's 26 sample\n",
      "n3k2d0.2's 27 sample\n",
      "n3k2d0.2's 28 sample\n",
      "n3k2d0.2's 29 sample\n",
      "n3k2d0.2's 30 sample\n",
      "n3k5d0.1's 1 sample\n",
      "n3k5d0.1's 2 sample\n",
      "n3k5d0.1's 3 sample\n",
      "n3k5d0.1's 4 sample\n",
      "n3k5d0.1's 5 sample\n",
      "n3k5d0.1's 6 sample\n",
      "n3k5d0.1's 7 sample\n",
      "n3k5d0.1's 8 sample\n",
      "n3k5d0.1's 9 sample\n",
      "n3k5d0.1's 10 sample\n",
      "n3k5d0.1's 11 sample\n",
      "n3k5d0.1's 12 sample\n",
      "n3k5d0.1's 13 sample\n",
      "n3k5d0.1's 14 sample\n",
      "n3k5d0.1's 15 sample\n",
      "n3k5d0.1's 16 sample\n",
      "n3k5d0.1's 17 sample\n",
      "n3k5d0.1's 18 sample\n",
      "n3k5d0.1's 19 sample\n",
      "n3k5d0.1's 20 sample\n",
      "n3k5d0.1's 21 sample\n",
      "n3k5d0.1's 22 sample\n",
      "n3k5d0.1's 23 sample\n",
      "n3k5d0.1's 24 sample\n",
      "n3k5d0.1's 25 sample\n",
      "n3k5d0.1's 26 sample\n",
      "n3k5d0.1's 27 sample\n",
      "n3k5d0.1's 28 sample\n",
      "n3k5d0.1's 29 sample\n",
      "n3k5d0.1's 30 sample\n",
      "n3k5d0.2's 1 sample\n",
      "n3k5d0.2's 2 sample\n",
      "n3k5d0.2's 3 sample\n",
      "n3k5d0.2's 4 sample\n",
      "n3k5d0.2's 5 sample\n",
      "n3k5d0.2's 6 sample\n",
      "n3k5d0.2's 7 sample\n",
      "n3k5d0.2's 8 sample\n",
      "n3k5d0.2's 9 sample\n",
      "n3k5d0.2's 10 sample\n",
      "n3k5d0.2's 11 sample\n",
      "n3k5d0.2's 12 sample\n",
      "n3k5d0.2's 13 sample\n",
      "n3k5d0.2's 14 sample\n",
      "n3k5d0.2's 15 sample\n",
      "n3k5d0.2's 16 sample\n",
      "n3k5d0.2's 17 sample\n",
      "n3k5d0.2's 18 sample\n",
      "n3k5d0.2's 19 sample\n",
      "n3k5d0.2's 20 sample\n",
      "n3k5d0.2's 21 sample\n",
      "n3k5d0.2's 22 sample\n",
      "n3k5d0.2's 23 sample\n",
      "n3k5d0.2's 24 sample\n",
      "n3k5d0.2's 25 sample\n",
      "n3k5d0.2's 26 sample\n",
      "n3k5d0.2's 27 sample\n",
      "n3k5d0.2's 28 sample\n",
      "n3k5d0.2's 29 sample\n",
      "n3k5d0.2's 30 sample\n",
      "n4k2d0.1's 1 sample\n",
      "n4k2d0.1's 2 sample\n",
      "n4k2d0.1's 3 sample\n",
      "n4k2d0.1's 4 sample\n",
      "n4k2d0.1's 5 sample\n",
      "n4k2d0.1's 6 sample\n",
      "n4k2d0.1's 7 sample\n",
      "n4k2d0.1's 8 sample\n",
      "n4k2d0.1's 9 sample\n",
      "n4k2d0.1's 10 sample\n",
      "n4k2d0.1's 11 sample\n",
      "n4k2d0.1's 12 sample\n",
      "n4k2d0.1's 13 sample\n",
      "n4k2d0.1's 14 sample\n",
      "n4k2d0.1's 15 sample\n",
      "n4k2d0.1's 16 sample\n",
      "n4k2d0.1's 17 sample\n",
      "n4k2d0.1's 18 sample\n",
      "n4k2d0.1's 19 sample\n",
      "n4k2d0.1's 20 sample\n",
      "n4k2d0.1's 21 sample\n",
      "n4k2d0.1's 22 sample\n",
      "n4k2d0.1's 23 sample\n",
      "n4k2d0.1's 24 sample\n",
      "n4k2d0.1's 25 sample\n",
      "n4k2d0.1's 26 sample\n",
      "n4k2d0.1's 27 sample\n",
      "n4k2d0.1's 28 sample\n",
      "n4k2d0.1's 29 sample\n",
      "n4k2d0.1's 30 sample\n",
      "n4k2d0.2's 1 sample\n",
      "n4k2d0.2's 2 sample\n",
      "n4k2d0.2's 3 sample\n",
      "n4k2d0.2's 4 sample\n",
      "n4k2d0.2's 5 sample\n",
      "n4k2d0.2's 6 sample\n",
      "n4k2d0.2's 7 sample\n",
      "n4k2d0.2's 8 sample\n",
      "n4k2d0.2's 9 sample\n",
      "n4k2d0.2's 10 sample\n",
      "n4k2d0.2's 11 sample\n",
      "n4k2d0.2's 12 sample\n",
      "n4k2d0.2's 13 sample\n",
      "n4k2d0.2's 14 sample\n",
      "n4k2d0.2's 15 sample\n",
      "n4k2d0.2's 16 sample\n",
      "n4k2d0.2's 17 sample\n",
      "n4k2d0.2's 18 sample\n",
      "n4k2d0.2's 19 sample\n",
      "n4k2d0.2's 20 sample\n",
      "n4k2d0.2's 21 sample\n",
      "n4k2d0.2's 22 sample\n",
      "n4k2d0.2's 23 sample\n",
      "n4k2d0.2's 24 sample\n",
      "n4k2d0.2's 25 sample\n",
      "n4k2d0.2's 26 sample\n",
      "n4k2d0.2's 27 sample\n",
      "n4k2d0.2's 28 sample\n",
      "n4k2d0.2's 29 sample\n",
      "n4k2d0.2's 30 sample\n",
      "n4k5d0.1's 1 sample\n",
      "n4k5d0.1's 2 sample\n",
      "n4k5d0.1's 3 sample\n",
      "n4k5d0.1's 4 sample\n",
      "n4k5d0.1's 5 sample\n",
      "n4k5d0.1's 6 sample\n",
      "n4k5d0.1's 7 sample\n",
      "n4k5d0.1's 8 sample\n",
      "n4k5d0.1's 9 sample\n",
      "n4k5d0.1's 10 sample\n",
      "n4k5d0.1's 11 sample\n",
      "n4k5d0.1's 12 sample\n",
      "n4k5d0.1's 13 sample\n",
      "n4k5d0.1's 14 sample\n",
      "n4k5d0.1's 15 sample\n",
      "n4k5d0.1's 16 sample\n",
      "n4k5d0.1's 17 sample\n",
      "n4k5d0.1's 18 sample\n",
      "n4k5d0.1's 19 sample\n",
      "n4k5d0.1's 20 sample\n",
      "n4k5d0.1's 21 sample\n",
      "n4k5d0.1's 22 sample\n",
      "n4k5d0.1's 23 sample\n",
      "n4k5d0.1's 24 sample\n",
      "n4k5d0.1's 25 sample\n",
      "n4k5d0.1's 26 sample\n",
      "n4k5d0.1's 27 sample\n",
      "n4k5d0.1's 28 sample\n",
      "n4k5d0.1's 29 sample\n",
      "n4k5d0.1's 30 sample\n",
      "n4k5d0.2's 1 sample\n",
      "n4k5d0.2's 2 sample\n",
      "n4k5d0.2's 3 sample\n",
      "n4k5d0.2's 4 sample\n",
      "n4k5d0.2's 5 sample\n",
      "n4k5d0.2's 6 sample\n",
      "n4k5d0.2's 7 sample\n",
      "n4k5d0.2's 8 sample\n",
      "n4k5d0.2's 9 sample\n",
      "n4k5d0.2's 10 sample\n",
      "n4k5d0.2's 11 sample\n",
      "n4k5d0.2's 12 sample\n",
      "n4k5d0.2's 13 sample\n",
      "n4k5d0.2's 14 sample\n",
      "n4k5d0.2's 15 sample\n",
      "n4k5d0.2's 16 sample\n",
      "n4k5d0.2's 17 sample\n",
      "n4k5d0.2's 18 sample\n",
      "n4k5d0.2's 19 sample\n",
      "n4k5d0.2's 20 sample\n",
      "n4k5d0.2's 21 sample\n",
      "n4k5d0.2's 22 sample\n",
      "n4k5d0.2's 23 sample\n",
      "n4k5d0.2's 24 sample\n",
      "n4k5d0.2's 25 sample\n",
      "n4k5d0.2's 26 sample\n",
      "n4k5d0.2's 27 sample\n",
      "n4k5d0.2's 28 sample\n",
      "n4k5d0.2's 29 sample\n",
      "n4k5d0.2's 30 sample\n",
      "n5k2d0.1's 1 sample\n",
      "n5k2d0.1's 2 sample\n",
      "n5k2d0.1's 3 sample\n",
      "n5k2d0.1's 4 sample\n",
      "n5k2d0.1's 5 sample\n",
      "n5k2d0.1's 6 sample\n",
      "n5k2d0.1's 7 sample\n",
      "n5k2d0.1's 8 sample\n",
      "n5k2d0.1's 9 sample\n",
      "n5k2d0.1's 10 sample\n",
      "n5k2d0.1's 11 sample\n",
      "n5k2d0.1's 12 sample\n",
      "n5k2d0.1's 13 sample\n",
      "n5k2d0.1's 14 sample\n",
      "n5k2d0.1's 15 sample\n",
      "n5k2d0.1's 16 sample\n",
      "n5k2d0.1's 17 sample\n",
      "n5k2d0.1's 18 sample\n",
      "n5k2d0.1's 19 sample\n",
      "n5k2d0.1's 20 sample\n",
      "n5k2d0.1's 21 sample\n",
      "n5k2d0.1's 22 sample\n",
      "n5k2d0.1's 23 sample\n",
      "n5k2d0.1's 24 sample\n",
      "n5k2d0.1's 25 sample\n",
      "n5k2d0.1's 26 sample\n",
      "n5k2d0.1's 27 sample\n",
      "n5k2d0.1's 28 sample\n",
      "n5k2d0.1's 29 sample\n",
      "n5k2d0.1's 30 sample\n",
      "n5k2d0.2's 1 sample\n",
      "n5k2d0.2's 2 sample\n",
      "n5k2d0.2's 3 sample\n",
      "n5k2d0.2's 4 sample\n",
      "n5k2d0.2's 5 sample\n",
      "n5k2d0.2's 6 sample\n",
      "n5k2d0.2's 7 sample\n",
      "n5k2d0.2's 8 sample\n",
      "n5k2d0.2's 9 sample\n",
      "n5k2d0.2's 10 sample\n",
      "n5k2d0.2's 11 sample\n",
      "n5k2d0.2's 12 sample\n",
      "n5k2d0.2's 13 sample\n",
      "n5k2d0.2's 14 sample\n",
      "n5k2d0.2's 15 sample\n",
      "n5k2d0.2's 16 sample\n",
      "n5k2d0.2's 17 sample\n",
      "n5k2d0.2's 18 sample\n",
      "n5k2d0.2's 19 sample\n",
      "n5k2d0.2's 20 sample\n",
      "n5k2d0.2's 21 sample\n",
      "n5k2d0.2's 22 sample\n",
      "n5k2d0.2's 23 sample\n",
      "n5k2d0.2's 24 sample\n",
      "n5k2d0.2's 25 sample\n",
      "n5k2d0.2's 26 sample\n",
      "n5k2d0.2's 27 sample\n",
      "n5k2d0.2's 28 sample\n",
      "n5k2d0.2's 29 sample\n",
      "n5k2d0.2's 30 sample\n",
      "n5k5d0.1's 1 sample\n",
      "n5k5d0.1's 2 sample\n",
      "n5k5d0.1's 3 sample\n",
      "n5k5d0.1's 4 sample\n",
      "n5k5d0.1's 5 sample\n",
      "n5k5d0.1's 6 sample\n",
      "n5k5d0.1's 7 sample\n",
      "n5k5d0.1's 8 sample\n",
      "n5k5d0.1's 9 sample\n",
      "n5k5d0.1's 10 sample\n",
      "n5k5d0.1's 11 sample\n",
      "n5k5d0.1's 12 sample\n",
      "n5k5d0.1's 13 sample\n",
      "n5k5d0.1's 14 sample\n",
      "n5k5d0.1's 15 sample\n",
      "n5k5d0.1's 16 sample\n",
      "n5k5d0.1's 17 sample\n",
      "n5k5d0.1's 18 sample\n",
      "n5k5d0.1's 19 sample\n",
      "n5k5d0.1's 20 sample\n",
      "n5k5d0.1's 21 sample\n",
      "n5k5d0.1's 22 sample\n",
      "n5k5d0.1's 23 sample\n",
      "n5k5d0.1's 24 sample\n",
      "n5k5d0.1's 25 sample\n",
      "n5k5d0.1's 26 sample\n",
      "n5k5d0.1's 27 sample\n",
      "n5k5d0.1's 28 sample\n",
      "n5k5d0.1's 29 sample\n",
      "n5k5d0.1's 30 sample\n",
      "n5k5d0.2's 1 sample\n",
      "n5k5d0.2's 2 sample\n",
      "n5k5d0.2's 3 sample\n",
      "n5k5d0.2's 4 sample\n",
      "n5k5d0.2's 5 sample\n",
      "n5k5d0.2's 6 sample\n",
      "n5k5d0.2's 7 sample\n",
      "n5k5d0.2's 8 sample\n",
      "n5k5d0.2's 9 sample\n",
      "n5k5d0.2's 10 sample\n",
      "n5k5d0.2's 11 sample\n",
      "n5k5d0.2's 12 sample\n",
      "n5k5d0.2's 13 sample\n",
      "n5k5d0.2's 14 sample\n",
      "n5k5d0.2's 15 sample\n",
      "n5k5d0.2's 16 sample\n",
      "n5k5d0.2's 17 sample\n",
      "n5k5d0.2's 18 sample\n",
      "n5k5d0.2's 19 sample\n",
      "n5k5d0.2's 20 sample\n",
      "n5k5d0.2's 21 sample\n",
      "n5k5d0.2's 22 sample\n",
      "n5k5d0.2's 23 sample\n",
      "n5k5d0.2's 24 sample\n",
      "n5k5d0.2's 25 sample\n",
      "n5k5d0.2's 26 sample\n",
      "n5k5d0.2's 27 sample\n",
      "n5k5d0.2's 28 sample\n",
      "n5k5d0.2's 29 sample\n",
      "n5k5d0.2's 30 sample\n",
      "n6k2d0.1's 1 sample\n",
      "n6k2d0.1's 2 sample\n",
      "n6k2d0.1's 3 sample\n",
      "n6k2d0.1's 4 sample\n",
      "n6k2d0.1's 5 sample\n",
      "n6k2d0.1's 6 sample\n",
      "n6k2d0.1's 7 sample\n",
      "n6k2d0.1's 8 sample\n",
      "n6k2d0.1's 9 sample\n",
      "n6k2d0.1's 10 sample\n",
      "n6k2d0.1's 11 sample\n",
      "n6k2d0.1's 12 sample\n",
      "n6k2d0.1's 13 sample\n",
      "n6k2d0.1's 14 sample\n",
      "n6k2d0.1's 15 sample\n",
      "n6k2d0.1's 16 sample\n",
      "n6k2d0.1's 17 sample\n",
      "n6k2d0.1's 18 sample\n",
      "n6k2d0.1's 19 sample\n",
      "n6k2d0.1's 20 sample\n",
      "n6k2d0.1's 21 sample\n",
      "n6k2d0.1's 22 sample\n",
      "n6k2d0.1's 23 sample\n",
      "n6k2d0.1's 24 sample\n",
      "n6k2d0.1's 25 sample\n",
      "n6k2d0.1's 26 sample\n",
      "n6k2d0.1's 27 sample\n",
      "n6k2d0.1's 28 sample\n",
      "n6k2d0.1's 29 sample\n",
      "n6k2d0.1's 30 sample\n",
      "n6k2d0.2's 1 sample\n",
      "n6k2d0.2's 2 sample\n",
      "n6k2d0.2's 3 sample\n",
      "n6k2d0.2's 4 sample\n",
      "n6k2d0.2's 5 sample\n",
      "n6k2d0.2's 6 sample\n",
      "n6k2d0.2's 7 sample\n",
      "n6k2d0.2's 8 sample\n",
      "n6k2d0.2's 9 sample\n",
      "n6k2d0.2's 10 sample\n",
      "n6k2d0.2's 11 sample\n",
      "n6k2d0.2's 12 sample\n",
      "n6k2d0.2's 13 sample\n",
      "n6k2d0.2's 14 sample\n",
      "n6k2d0.2's 15 sample\n",
      "n6k2d0.2's 16 sample\n",
      "n6k2d0.2's 17 sample\n",
      "n6k2d0.2's 18 sample\n",
      "n6k2d0.2's 19 sample\n",
      "n6k2d0.2's 20 sample\n",
      "n6k2d0.2's 21 sample\n",
      "n6k2d0.2's 22 sample\n",
      "n6k2d0.2's 23 sample\n",
      "n6k2d0.2's 24 sample\n",
      "n6k2d0.2's 25 sample\n",
      "n6k2d0.2's 26 sample\n",
      "n6k2d0.2's 27 sample\n",
      "n6k2d0.2's 28 sample\n",
      "n6k2d0.2's 29 sample\n",
      "n6k2d0.2's 30 sample\n",
      "n6k5d0.1's 1 sample\n",
      "n6k5d0.1's 2 sample\n",
      "n6k5d0.1's 3 sample\n",
      "n6k5d0.1's 4 sample\n",
      "n6k5d0.1's 5 sample\n",
      "n6k5d0.1's 6 sample\n",
      "n6k5d0.1's 7 sample\n",
      "n6k5d0.1's 8 sample\n",
      "n6k5d0.1's 9 sample\n",
      "n6k5d0.1's 10 sample\n",
      "n6k5d0.1's 11 sample\n",
      "n6k5d0.1's 12 sample\n",
      "n6k5d0.1's 13 sample\n",
      "n6k5d0.1's 14 sample\n",
      "n6k5d0.1's 15 sample\n",
      "n6k5d0.1's 16 sample\n",
      "n6k5d0.1's 17 sample\n",
      "n6k5d0.1's 18 sample\n",
      "n6k5d0.1's 19 sample\n",
      "n6k5d0.1's 20 sample\n",
      "n6k5d0.1's 21 sample\n",
      "n6k5d0.1's 22 sample\n",
      "n6k5d0.1's 23 sample\n",
      "n6k5d0.1's 24 sample\n",
      "n6k5d0.1's 25 sample\n",
      "n6k5d0.1's 26 sample\n",
      "n6k5d0.1's 27 sample\n",
      "n6k5d0.1's 28 sample\n",
      "n6k5d0.1's 29 sample\n",
      "n6k5d0.1's 30 sample\n",
      "n6k5d0.2's 1 sample\n",
      "n6k5d0.2's 2 sample\n",
      "n6k5d0.2's 3 sample\n",
      "n6k5d0.2's 4 sample\n",
      "n6k5d0.2's 5 sample\n",
      "n6k5d0.2's 6 sample\n",
      "n6k5d0.2's 7 sample\n"
     ]
    }
   ],
   "source": [
    "exp = test_heuristics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_alpha_zero():\n",
    "    if not os.path.isfile('./result/alpha_zero_experiment_result.pkl'):\n",
    "        with open('./result/alpha_zero_experiment_result.pkl', 'rb') as fp:\n",
    "            exp = load(fp)\n",
    "        with open('./result/c7.csv', 'w', newline='') as fp:\n",
    "            writer = csv.writer(fp)\n",
    "            for n in range(5 - 3, 11 - 3):\n",
    "                size_value = exp[n][1][0][1] / 100\n",
    "                writer.writerow([size_value])\n",
    "    else:\n",
    "        data = load_data('nfa')\n",
    "        exp = [[[[0, 0] for d in range(len(density))] for k in range(\n",
    "            len(alphabet))] for n in range(n_range)]\n",
    "        g = Game()\n",
    "        nnet = nn(g)\n",
    "        mcts = MCTS(g, nnet, args)\n",
    "        def player(x): return np.argmax(mcts.getActionProb(x, temp=0))\n",
    "        curPlayer = 1\n",
    "        if args.load_model:\n",
    "            nnet.load_checkpoint(args.checkpoint, args.load_folder_file[1])\n",
    "        else:\n",
    "            print(\"Can't test without pre-trained model\")\n",
    "            exit()\n",
    "        for n in range(n_range):\n",
    "            for k in range(len(alphabet)):\n",
    "                for d in range(len(density)):\n",
    "                    for i in range(sample_size):\n",
    "                        # print('n' + str(n + min_n) + 'k' + ('2' if not k else ('5' if k == 1 else '10')) + (\n",
    "                        #    's' if not d else 'd') + '\\'s ' + str(i + 1) + ' sample')\n",
    "                        gfa = data[n][k][d][i].dup()\n",
    "                        board = g.getInitBoard(\n",
    "                            gfa, n + min_n, alphabet[k], density[d])\n",
    "                        order = []\n",
    "                        start_time = time.time()\n",
    "                        while g.getGameEnded(board, curPlayer) == -1:\n",
    "                            action = player(\n",
    "                                g.getCanonicalForm(board, curPlayer))\n",
    "                            valids = g.getValidMoves(\n",
    "                                g.getCanonicalForm(board, curPlayer), 1)\n",
    "                            if valids[action] == 0:\n",
    "                                assert valids[action] > 0\n",
    "                            board, curPlayer = g.getNextState(\n",
    "                                board, curPlayer, action)\n",
    "                            order.append(action)\n",
    "\n",
    "                        result = gfa.delta[0][1].treeLength()\n",
    "                        end_time = time.time()\n",
    "                        # gfa.eliminateAll(order)\n",
    "                        '''\n",
    "                        if (result != gfa.delta[0][n + min_n + 1].treeLength()):\n",
    "                            print('order', order)\n",
    "                            print('result length', result)\n",
    "                            print('valid length',\n",
    "                                  gfa.delta[0][n + min_n + 1].treeLength())\n",
    "                            print('Something is wrong')\n",
    "                            exit()\n",
    "                        '''\n",
    "                        result_time = end_time - start_time\n",
    "                        exp[n][k][d][0] += result_time\n",
    "                        exp[n][k][d][1] += result\n",
    "        with open('./result/alpha_zero_experiment_result.pkl', 'wb') as fp:\n",
    "            dump(exp, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for StateEliminationNNet:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([64, 65, 5, 5]) from checkpoint, the shape in current model is torch.Size([64, 1, 5, 5]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2003242/3788159005.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_alpha_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2003242/2218184225.py\u001b[0m in \u001b[0;36mtest_alpha_zero\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcurPlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_folder_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't test without pre-trained model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Experiments/FAtoRegex/SHORT/alpha_zero/state_elimination/pytorch/NNet.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(self, folder, filename)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1605\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for StateEliminationNNet:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([64, 65, 5, 5]) from checkpoint, the shape in current model is torch.Size([64, 1, 5, 5])."
     ]
    }
   ],
   "source": [
    "test_alpha_zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./result/alpha_zero_experiment_result.pkl', 'rb') as fp:\n",
    "    exp_alpha = np.array(load(fp)) / sample_size\n",
    "\n",
    "with open('./result/heuristics_experiment_result.pkl', 'rb') as fp:\n",
    "    exp_heuristic = np.array(load(fp)) / sample_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.0005883455276489257, 10.4],\n",
       "  [0.0008560260136922201, 26.233333333333334],\n",
       "  [0.0012173811594645182, 60.5],\n",
       "  [0.0019341627756754556, 167.8],\n",
       "  [0.002600574493408203, 318.06666666666666]],\n",
       " [[0.04385823408762614, 9.766666666666667],\n",
       "  [0.10055326620737712, 28.633333333333333],\n",
       "  [0.1980108340581258, 86.96666666666667],\n",
       "  [0.33572565714518227, 203.93333333333334],\n",
       "  [0.47935725847880045, 375.8]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(exp_heuristic[5][:6])[:, 1, 0].tolist(), np.array(exp_alpha[:6])[:, 1, 0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
